Feature Selection Mini-Project

Katie explained in a video a problem that arose in preparing Chris and Sara’s email for the author identification project; it had to do with a feature that was a little too powerful (effectively acting like a signature, which gives an arguably unfair advantage to an algorithm). You’ll work through that discovery process here.

Overfitting a Decision Tree 1
This bug was found when Katie was trying to make an overfit decision tree to use as an example in the decision tree mini-project. A decision tree is classically an algorithm that can be easy to overfit; one of the easiest ways to get an overfit decision tree is to use a small training set and lots of features.
If a decision tree is overfit, would you expect the accuracy on a test set to be very high or pretty low?
SKIP TO QUIZ


Overfitting a Decision Tree 2
If a decision tree is overfit, would you expect high or low accuracy on the training set?

Number of Features and Overfitting
A classic way to overfit an algorithm is by using lots of features and not a lot of training data. You can find the starter code in feature_selection/find_signature.py. Get a decision tree up and training on the training data, and print out the accuracy. How many training points are there, according to the starter code?

Quiz: Number Of Features And Overfitting
Special Note: Depending on when you downloaded the code provided for find_signature.py, you may need to change the code in lines 9-10 to be
words_file = "../text_learning/your_word_data.pkl" authors_file = "../text_learning/your_email_authors.pkl"
so that the files created from running vectorize_text.py are reflected properly.

In addition, if you are having trouble getting the code to run due to memory issues, then if you are on version 0.16.x of scikit-learn, you can remove the .toarray() function from the line where features_train is created to save on memory - the decision tree classifier can, in that version take as input a sparse array instead of only dense arrays.

Accuracy of Your Overfit Decisio
What’s the accuracy of the decision tree you just made? (Remember, we're setting up our decision tree to overfit -- ideally, we want to see the test accuracy as relatively low.)

Identify the Most Powerful Featu
Take your (overfit) decision tree and use the featureimportances attribute to get a list of the relative importance of all the features being used. We suggest iterating through this list (it’s long, since this is text data) and only printing out the feature importance if it’s above some threshold (say, 0.2--remember, if all words were equally important, each one would give an importance of far less than 0.01). What’s the importance of the most important feature? What is the number of this feature?

Quiz: Identify The Most Powerful Features
Special Note: Depending on when you downloaded the code provided for find_signature.py, you may need to change the code in lines 9-10 to be
words_file = "../text_learning/your_word_data.pkl" authors_file = "../text_learning/your_email_authors.pkl"
so that the files created from running vectorize_text.py are reflected properly.

Use TfIdf to Get the Most Import
In order to figure out what words are causing the problem, you need to go back to the TfIdf and use the feature numbers that you obtained in the previous part of the mini-project to get the associated words. You can return a list of all the words in the TfIdf by calling get_feature_names() on it; pull out the word that’s causing most of the discrimination of the decision tree. What is it? Does it make sense as a word that’s uniquely tied to either Chris Germany or Sara Shackleton, a signature of sorts?
SKIP TO QUIZ


Quiz: Use TfIdf To Get The Most Important Word
Make sure you write your modifications to find_signature.py to obtain the most impactful word.

Remove, Repeat
This word seems like an outlier in a certain sense, so let’s remove it and refit. Go back to text_learning/vectorize_text.py, and remove this word from the emails using the same method you used to remove “sara”, “chris”, etc. Rerun vectorize_text.py, and once that finishes, rerun find_signature.py. Any other outliers pop up? What word is it? Seem like a signature-type word? (Define an outlier as a feature with importance >0.2, as before).

Quiz: Remove, Repeat
Special Note: Depending on when you downloaded the code provided for find_signature.py, you may need to change the code in lines 9-10 to be
words_file = "../text_learning/your_word_data.pkl" authors_file = "../text_learning/your_email_authors.pkl"
so that the files created from running vectorize_text.py are reflected properly.

Checking Important Features Ag
Update vectorize_test.py one more time, and rerun. Then run find_signature.py again. Any other important features (importance>0.2) arise? How many? Do any of them look like “signature words”, or are they more “email content” words, that look like they legitimately come from the text of the messages?

Quiz: Checking Important Features Again

Accuracy of the Overfit Tree
What’s the accuracy of the decision tree now? We've removed two "signature words", so it will be more difficult for the algorithm to fit to our limited training set without overfitting. Remember, the whole point was to see if we could get the algorithm to overfit--a sensible result is one where the accuracy isn't that great!

Quiz: Accuracy Of The Overfit Tree




